\documentclass{beamer}
\usepackage{graphicx}
\input{../s20xPreambleRBM.tex}
\begin{document}
\newcommand{\thechapter}{6}

<<RC-H06-000, echo=FALSE>>=
source("../s20xNotesHelper.R")

## these are global knitr options and settings for the
## whole document
library(knitr)
library(s20x)
## comment = NA removes ## from all output lines
## prompt = TRUE means the console input prompt > is displayed
## tidy = TRUE means the code is properly spaced and tidied. 
opts_chunk$set(comment = NA, size = "scriptsize", prompt = TRUE, tidy = TRUE)
@

\title{Chapter 6: \\ Multiplicative linear models}
\institute{University of Auckland}

\begin{frame}
\titlepage
\end{frame}


\begin{frame}[t]
\frametitle{Learning Outcomes}
In this chapter you will learn about:
\begin{center}
\vspace{16pt}
\begin{minipage}{0.9\textwidth}
  \begin{itemize}
  \item Mean versus median -- which to use?
  \item Transforming the response variable using the log function
  \item Multiplicative models
  \item Why inference is about the median (not the mean)
  \item A multiplicative simple linear regression example
  \item A multiplicative two-sample t-test example
  \item Relevant \rcode{R}-code.
  \end{itemize}
\end{minipage}
\end{center}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\BeginSection{Mean versus median -- which to use?}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{frame}[t]
\frametitle{Auckland house prices}
In early 2021 one of the STATS 20x lecturers was looking to buy a house in a working-class suburb of Auckland, about 15 minutes by train to downtown.\footnote{Auckland house prices are very high by international standards.}
\medskip

The lecturer downloaded 94 recent sales prices for the suburb, and put the prices (\$1000's) in a text file called \rcode{AkldHousePrices.txt}. The lecturer just wants to know the typical house price in the suburb -- there are no explanatory variables.
\bigskip

It sounds like an easy analysis. We just need to fit a null model. Let's take a look...
\end{frame}



\begin{frame}[fragile]
\frametitle{Auckland house prices\ldots}
\framesubtitle{Inspect the data}
<<RC-H06-001, fig.show = 'hide',tidy=F>>=
Houses.df=read.table("Data/AkldHousePrices.txt",header=T)
hist(Houses.df$price, breaks=20,main="",xlab="Price ($1000)")
abline(v = c(mean(Houses.df$price), median(Houses.df$price)),
       col = c("blue", "green"), lwd = 2)
@

<<RC-H06-002, echo = FALSE>>=
trimPlot(Houses.df$price,
         fileName = "figure/RC-H06-002.pdf", 
         plotCommand = hist, 
         main = "", breaks=20,
         x.lab = "Price ($1000)",
         y.lab = "Frequency",
         fig.height = 2.25,
         fig.width = 4.0,
         addElements = list(
           abline(v = c(mean(Houses.df$price), median(Houses.df$price)), col = c("blue", "green"), lwd = 2),
           legend("topright", lty = 1, lwd = 2, legend = c("Mean", "Median"), col = c("blue", "green"), bty = "n", cex = 0.7)
         ))
@

\begin{figure}
  \centering
  \includegraphics{figure/RC-H06-002}
\end{figure}

\end{frame}



\begin{frame}[fragile,t]
\frametitle{Auckland house prices\ldots}
\framesubtitle{Inspect the data...}
<<RC-H06-003>>=
summary(Houses.df$price)
@
\bigskip
Clearly, we are not dealing with data that come from a normal distribution.  See how the (sample) median is markedly lower than the (sample) mean. \\
\bigskip 
This type of right-skew distribution is very common when it comes to things involving money
(\$\$\$), resources, growth, salary, age, advantage and energy, to name but a few.
\bigskip

We can still make inference using a linear model for the expected house price because we can apply the \textbf{CLT} since we have a largish number of observations ($n=94$).
Alternatively, we can use the bootstrap since it does not require the assumption of normality.
\end{frame}




\begin{frame}[fragile]
\frametitle{Auckland house prices\ldots}
\framesubtitle{Inference about the mean}
Here is histogram of 10,000 bootstrap sample means from resampling the house price data.

<<RC-H06-004, echo=FALSE>>=
library(bootstrap)
bootstrappedMeanPrices = bootstrap(Houses.df$price, 10000, mean)$thetastar
bootstrappedMedianPrices = bootstrap(Houses.df$price, 10000, median)$thetastar

trimPlot(bootstrappedMeanPrices,
         fileName = "figure/RC-H06-004.pdf",
         plotCommand = hist,
         main = "",
         x.lab = "Sample means of bootstrapped house prices ($1000)",
         y.lab = "Frequency",
         col = "lightblue",
         fig.width = 4,
         fig.height = 2)
@

\begin{figure}
  \centering
  \includegraphics{figure/RC-H06-004}
\end{figure}

The approximate normality ot the bootstrapped sample means shows that the sample mean has 
an approximate normal distribution.
\end{frame}



\begin{frame}[fragile]
\frametitle{Auckland house prices\ldots}
\framesubtitle{Inference  about the mean\ldots}

Here is the bootstrap 95\% CI for the expected price, along with output from the null model.

<<RC-H06-005>>=
quantile(bootstrappedMeanPrices, c(.025, .975))
@

<<RC-H06-006, results='hide'>>=
HousesNull.fit=lm(price~1, data=Houses.df)
summary(HousesNull.fit)
@

<<RC-H06-007, echo=FALSE>>=
slimSummary(HousesNull.fit)
@

<<RC-H06-008>>=
confint(HousesNull.fit)
@

Here we see that, to within less than NZ\$10\,000,  the bootstrap CI and the model CI are the same. This is confirmation that the \textbf{CLT}  works.
\end{frame}


\begin{frame}[fragile]
\frametitle{Auckland house prices\ldots}
\framesubtitle{Inference  about the mean\ldots}
So we can safely say that the mean (i.e., expected) sale price for the entire suburb is somewhere between, say, NZ\$1.17 million to NZ\$1.45 million.
However, the gross non-normality of the data prevents us from making prediction intervals for house prices. 

\medskip Moreover, the estimated mean price of NZ\$1.31 million is somewhat misleading.\footnote{It is also not statistically robust since it is very sensitive to the number of expensive ($\geq$ \$3 million, say) houses sold}
Our house-hunting lecturer just wants a typical house, about midway in affordability at most. It would make more sense to estimate the {\bf median} house price, since our lecturer will then know that half of the houses going on the property market will sell for less than that price.
\medskip

In general, for highly skewed data the median is usually a better measure of `typical' value than the mean.
\end{frame}



\begin{frame}[fragile]
\frametitle{Auckland house prices\ldots}
\framesubtitle{Inference about the median}

To estimate the median sale price of the entire suburb the natural estimate is the median of our sample:
<<RC-H06-009a>>=
median(Houses.df$price)
@
and we can use a bootstrap to get a 95\% CI for the suburb median
<<RC-H06-009b>>=
quantile(bootstrappedMedianPrices, c(.025, .975))
@
so we can say that the median sale price for the entire suburb is somewhere between,
say, NZ\$1.04 million to NZ\$1.32 million.
\bigskip

The 95\% CI for the median comes as something of a relief to our house-hunting lecturer. It is much more reasonable than the \$1.17 to \$1.45 million CI for the mean which was markedly higher due to the data being so right-skewed. 
\end{frame}


\begin{frame}[fragile]
\frametitle{Inference about the median}

In the above house price example we are working with iid data, 
so it is natural to use the sample median to estimate the population median.
\bigskip \bigskip

We'll see in the next section that the linear model framework can also be used to make
inference about the median provided that the logged response variable is approximately normally distributed.
This approach has the advantage that it can also be applied to more general situations where we have explanatory variables that may be associated with the response variable. 
\medskip

We'll also see that fitting linear models to logged response data results in the effects of explanatory variables acting multiplicatively on medians.

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\BeginSection{Transforming the response variable using the log function}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]
\frametitle{Auckland house prices\ldots}
\framesubtitle{Transforming the data}
Let's consider making a transformation of the prices. In particular, the log transformation.
Here is the histogram of \rcode{log(price)}.

<<RC-H06-010, echo=FALSE>>=
trimPlot(log(Houses.df$price),
         fileName = "figure/RC-H06-010.pdf",
         plotCommand = hist,
         main = "",
         x.lab = "log(Price)",
         y.lab = "Frequency",
         fig.width = 4.5,
         fig.height = 2.0)
@

\begin{figure}
  \centering
  \includegraphics{figure/RC-H06-010}
\end{figure}

This looks reasonably close to normal, so if we fit a linear model to these data then all inferences will be valid.
\end{frame}



\begin{frame}[fragile]
\frametitle{Auckland house prices\ldots}
\framesubtitle{Null model fitted to logged house price data}
<<RC-H06-012>>=
LoggedPriceNull.fit=lm(log(price)~1, data=Houses.df)
coef(summary(LoggedPriceNull.fit))
@

<<RC-H06-014>>=
confint(LoggedPriceNull.fit)
@

Well that is interesting, but logged house prices don't mean much to anyone who is hoping to buy a house. The inference needs to be back-transformed to NZ\$. 

Since we've used the log transformation, the back-transformation is the exponential function \rcode{exp()}.

<<RC-H06-016>>=
exp(confint(LoggedPriceNull.fit))
@

\end{frame}



\begin{frame}[fragile]
\frametitle{Auckland house prices\ldots}
\framesubtitle{The effect of transforming}
The above confidence interval is quite different from the one we calculated for the mean house price of the suburb. The reason for this is because the above CI is for the {\bf median} house price. 
\medskip

To see why this is, let's take a look at what happens when we transform summary statistics using the \rcode{log()} and \rcode{exp()} functions: 
\medskip

Summaries of price:
<<RC-H06-0111a>>=
summary(Houses.df$price);
@
Summaries of log(price):
<<RC-H06-0111b>>=
summary(log(Houses.df$price))
@
Back-transformed summaries of log(price):
<<RC-H06-0111c>>=
exp(summary(log(Houses.df$price)))
@

%As a result the summary values that just depend on the ordering of the values (e.g., minimum, %lower quartile, median, upper quartile, maximum) statistics are not affected.\footnote{The %order statistics for a sample are those that just depend on the order of the observations.} 
\end{frame}



\begin{frame}[fragile]
\frametitle{Auckland house prices\ldots}
\framesubtitle{The effect of transforming\ldots}

Note that the sample mean changed after transforming then back-transforming, but the other summary values did not. This is because transforming and back-transforming does not change the order of the data -- the smallest value will still be the smallest value, the middle value will still be the middle value.

\end{frame}



\begin{frame}[fragile]
\frametitle{Auckland house prices\ldots}
\framesubtitle{Why inference is about the population median}
If the logged response variable is normally distributed, then on the log scale its population mean and median are exactly the same number.\footnote{The sample mean and median may differ a little, as we saw with the logged house prices} 
In that case, a 95\% confidence interval for the population mean of the logged response variable is also a 95\% CI for the population median of the logged response variable.
\medskip

Since the median is unaffected by transforming and back-transforming, it follows that back-transforming the above 95\% confidence interval will give a 95\% confidence interval for the population median on the original scale. However, it will not be a 95\% confidence interval for the population mean.
\medskip

We now have a recipe for calculating a confidence interval for the {\bf median} -- fit a linear model to the log-transformed data, calculate a 95\% confidence interval for the {\bf mean} (and hence {\bf median}), and back transform.
\end{frame}


\begin{frame}[fragile]
\frametitle{Auckland house prices\ldots}
\framesubtitle{Inference about the median}
Our back-transformed estimate ($\exp(\hat{\beta}_0)$) 
and 95\% CI for the median suburb sale price are
<<RC-H06-016b>>=
exp(coef(LoggedPriceNull.fit))
exp(confint(LoggedPriceNull.fit))
@

So we can say we are reasonably sure (95\% confident) that the median house price is somewhere between NZ\$1.06 and NZ\$1.29 million.
\bigskip

These values differ a little from the sample median (NZ\$1130) and the 95\% bootstrap CI
of NZ\$1040 to NZ\$1320 we saw in the previous section. 
This is because the machinery being used is different.

\bigskip

 
\bigskip

\end{frame}



\begin{frame}
\frametitle{Auckland house prices\ldots}
\framesubtitle{Inference for the house price data}

Here is the confidence interval for the mean suburb price with the sample mean shown in blue.
<<RC-H06-017, echo=FALSE>>=
fit1 = lm(price~1, data = Houses.df)
mx = mean(Houses.df$price)
bds = confint(fit1)

trimPlot(Houses.df$price,
         fileName = "figure/RC-H06-017.pdf",
         plotCommand = hist,
         main = "", breaks=20,
         x.lab = "Price ($1000)",
         y.lab = "Frequency",
         fig.width = 4.25,
         fig.height = 2.5,
         addElements = list(
            abline(v=mx, lwd=1, col = "#377eb8"),
            abline(v=bds, lwd = 1, lty = 5, col ="#e41a1c")
         ))
@

\begin{figure}
  \centering
  \includegraphics{figure/RC-H06-017}
\end{figure}

\end{frame}


\begin{frame}
\frametitle{Auckland house prices\ldots}
\framesubtitle{Inference for the house price data\ldots}

Here is the confidence interval for the median suburb price with the estimate obtained from the linear model approach ($\exp(\hat{\beta}_0)$) shown in green.
<<RC-H06-018, echo=FALSE>>=
fit2 = lm(log(price)~1, data = Houses.df)
bds = exp(confint(fit2))

trimPlot(Houses.df$price,
         fileName = "figure/RC-H06-018.pdf",
         plotCommand = hist,
         main = "",breaks=20,
         x.lab = "Price ($1000)",
         y.lab = "Frequency",
         fig.width = 4.25,
         fig.height = 2.5,
         addElements = list(
            abline(v=exp(coef(fit2)), lwd=1, col = "#4daf4a"),
            abline(v=bds, lwd = 1, lty = 5, col ="#984ea3")
         ))
@

\begin{figure}
  \centering
  \includegraphics{figure/RC-H06-018}
\end{figure}

\end{frame}


\begin{frame}
\frametitle{Auckland house prices\ldots}
\framesubtitle{Inference for the house price data\ldots}

Here is the above plot with the sample mean and sample median shown as well.
<<RC-H06-019, echo=FALSE>>=
trimPlot(Houses.df$price,
         fileName = "figure/RC-H06-019.pdf",
         plotCommand = hist,
         main = "",breaks=20,
         x.lab = "Price ($1000)",
         y.lab = "Frequency",
         fig.width = 4.25,
         fig.height = 2.25,
         addElements = list(
            abline(v=exp(coef(fit2)), lwd=1, col = "#4daf4a"),
            abline(v=bds, lwd = 1, lty = 5, col ="#984ea3"),
            abline(v=median(Houses.df$price), lwd = 1, col ="darkorange"),
            abline(v=mean(Houses.df$price), lwd = 1, col ="#377eb8"),
            legend("topright", lty = 1, lwd = 2, legend = c(expression("exp("*hat(beta)[0]*")"), "Sample Median", "Sample Mean"), col = c("#4daf4a", "darkorange", "#377eb8"), bty = "n", cex = 0.7)
         ))
@

\begin{figure}
  \centering
  \includegraphics{figure/RC-H06-019}
\end{figure}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\BeginSection{The log function turns multiplicative effects in to additive effects}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{frame}[fragile]
\frametitle{Why the log transformation?}
In the above house price example it would have been possible to use many other choices for our transformation of the prices. How did we know that the log-transformation would transform the data to be approximately normally distributed? We didn't! But, the log transformation has a very special property that makes it a very sensible choice.
{\bf The log transformation turns multiplicative effects in to additive effects.}
\medskip

In many situations, the response variable is subject to effects that act multiplicatively. E.g., would proximity to a train station act additively or multiplicatively on the price of a house?\footnote{In other words, does being close to the train station make a house worth, say, \$150,000 more, or 10\% more?} What about having a view or being beside the estuary?
\medskip

In such cases, taking the log of the response variable results in these effects acting additively on the log scale...and by virtue of the CLT, the consequence of a lot of effects adding together is approximate normality on the log scale.

\end{frame}

\begin{frame}[fragile]
\frametitle{Logarithm refresher}
\framesubtitle{Turning multiplication in to addition}
You have used logarithms before.
When you multiply $100$ by $100,000$ you add the zeroes to create $10,000,000$.
That is, two $0$s + five $0$s = seven $0$s = $10,000,000$. 
More formally:
\[ 100\times 100,000 =10^{2}\times 10^{5}=10^{2+5}=10^7=10,000,000 \] \\[-5mm]

Using base 10 (the number of fingers/toes we humans have) we can write $log_{10}({100})=log_{10}({10^2})=2$.
So , 
\begin{align*}
log_{10}(100\times 100,000)&=log_{10}(10^2\times 10^5) \\
                        &=log_{10}(10^{2+5})=2+5=7 \\
                        &=log_{10}(100)+log_{10}(100,000) \ .\\
\end{align*}
\vspace{-30pt}

So we see that the log of a product (i.e., multiplication) is the sum (i.e, addition) of the logs. 

\begin{columns}
\begin{column}{0.85\textwidth}
{\scriptsize In Springfield, they {\em should} work in base 8.
This means that``100'' in Springfield would correspond to our 64.}
\end{column}
\begin{column}{0.15\textwidth}
\includegraphics[width=0.6in]{BartSimpson.jpg}
\end{column}
\end{columns}
\end{frame}



\begin{frame}[fragile]
\frametitle{Logarithm refresher\ldots}
\frametitle{Why use base $e$?}

In the scientific world we use base $e$ logarithms\footnote{$e=\lim_{n\to\infty}(1+\frac{1}{n})^{n}= 2.718282\ldots$} ($\log_e$) which are referred to as natural logs\footnote{Some programming languages and books use $\ln$ for the $\log_e$ function.}. 
\bigskip

In \rcode{R} the \rcode{log} function is the natural logarithm. If you need to calculate the logarithm base 10, then you can use the \rcode{log10} function, or type \rcode{log(x, base = 10)}, where \rcode{x} is the value for which you are trying to calculate the logarithm.\\
\bigskip
In many situations, it does not matter which base you use as the laws of logarithms hold regardless of the base you use. That is,
\[
\log_b(x\times y)=\log_b(x)+\log_b(y)
\]
for any base $b>0$, and values $x,y>0$.
\end{frame}


\begin{frame}[fragile]
\frametitle{The inverse log function, $e^x$}
\framesubtitle{Turning addition in to multipication\ldots}

Because we use natural logs ($\log_e$), it follows that the inverse transformation (i.e., back transformation) is the exponential function. This is the \rcode{exp} function in {\rcode R}. 
\medskip

Exponentiating a sum is equivalent to multiplying the separate exponentials. 
That is,
\[ e^{x+y} = e^x \times e^y \]
For example:

<<RC-H06-019b>>=
exp(2)
exp(3)
exp(2+3)
exp(2)*exp(3)
@
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\BeginSection{Example 1: Multiplicative simple linear regression model}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{frame}[fragile]
\frametitle{Multiplicative model with numerical explanatory variable}
\framesubtitle{Mazda price data}
When we also have explanatory variables ($x$), using the log transformation (applied to the response variable $y$) changes the shape of the relationship between $x$ and $y$ from additive to multiplicative. This is because we use the exponential to back-transform our additive linear model fitted to $\log(y)$.
\bigskip

We demonstrate with a new example: \\
\medskip
The year of manufacturer and asking price of 123 Mazda cars were collected from the Melbourne Age newspaper in 1991. The variables measured were:
\bigskip

\begin{center}
\begin{tabular}{ll}
\rcode{price}& price of vehicle in Australian \$ \\
\rcode{year}&	year of manufacture ($1990 = 90$) \\
\end{tabular}
\end{center}
\bigskip

We will assume that these data are a random sample from the population.\footnote{What would the population be here?}
\end{frame}


\begin{frame}[fragile]
\frametitle{Multiplicative model with numerical explanatory variable}
\framesubtitle{Depreciation of Mazda cars}
Here we wish to understand how the cars lose value as they age. We have the year the car was manufactured so we can ascertain its age since these data were collected in 1991 (or '91). That is, \rcode{age=91-year}. \\
\bigskip
\textbf{Intuition:} What does common sense tell us about the shape of the relationship between car price and age? That is, what do we \emph{expect} to see?
\bigskip

\begin{itemize}
\item An additive decrease in price with age? 
\item A multiplicative decrease in price with age?
\end{itemize}
\end{frame}


\begin{frame}[fragile]
\frametitle{Multiplicative model with numerical explanatory variable}
\framesubtitle{Depreciation of Mazda cars\ldots}
<<RC-H06-020, fig.show='hide'>>=
Mazda.df = read.table("Data/mazda.txt",header=T)
Mazda.df$age = 91 - Mazda.df$year ## Create the age variable
plot(price~age, data = Mazda.df, xlab = "Age", ylab = "Price (AUD)")
@

<<RC-H06-021, echo=FALSE>>=
trimPlot(price ~ age,
         data = Mazda.df,
         fileName = "figure/RC-H06-021.pdf",
         x.lab = "Age",
         y.lab = "Price (AUD)",
         fig.height = 2.25,
         fig.width = 4.25,
         cex = 0.7,
         axis.size.cex = c(0.7, 0.525))
@

\begin{figure}
  \centering
  \includegraphics{figure/RC-H06-021}
\end{figure}

\end{frame}


\begin{frame}[fragile]
\frametitle{Multiplicative model with numerical explanatory variable}
\framesubtitle{Depreciation of Mazda cars\ldots}
Let us look at the two components we are interested in, trend and scatter. 

<<RC-H06-022, echo=FALSE>>=
trimPlot(price ~ age,
         plotCommand = trendscatter,
         fileName = "figure/RC-H06-022.pdf",
         data = Mazda.df,
         main = "",
         x.lab = "Age",
         y.lab = "Price (AUD)",
         fig.height = 2.6,
         fig.width = 4.5,
         cex = 0.7,
         axis.size.cex = c(0.7, 0.525))
@

\begin{figure}
  \centering
  \includegraphics{figure/RC-H06-022}
\end{figure}

% Here we see increased scatter with higher prices and a non-linear
% relationship between age and price.
\end{frame}


\begin{frame}[fragile]
\frametitle{Multiplicative model with numerical explanatory variable}
\framesubtitle{Depreciation of Mazda cars: A na\"ive price vs age models}
The trend is decreasing (exponentially), along with decreasing scatter -- these are classic symptoms of an underlying multiplicative model. Assuming \textbf{EOV} would be na\"ive in this case. Let us be na\"ive and see where it takes us. Let us fit a a linear model and see what the residual plot tells us.

\end{frame}


\begin{frame}[fragile]
\frametitle{Multiplicative model with numerical explanatory variable}
\framesubtitle{Na\"ive price vs age models\ldots}

<<RC-H06-023, fig.show='hide'>>=
PriceAge.fit=lm(price~age, data=Mazda.df)
plot(PriceAge.fit,which=1)
@

<<RC-H06-024, echo=FALSE>>=
trimPlot(PriceAge.fit,
         fileName = "figure/RC-H06-024.pdf",
         which = 1,
         cex = 0.7,
         cex.main = 0.7,
         fig.height = 2.5,
         fig.width = 4.5,
         x.lab = "Fitted values",
         y.lab = "Residuals",
         axis.size.cex = c(0.7, 0.525))
@

\begin{figure}
  \centering
  \includegraphics{figure/RC-H06-024}
\end{figure}

\end{frame}


\begin{frame}[fragile]
\frametitle{Multiplicative model with numerical explanatory variable}
\framesubtitle{Na\"ive price vs age models\ldots}
The decreasing non-linear trend and non-constant scatter has become even more apparent.
Note: the higher fitted values on the horizontal axis are associated with newer cars. 
Let us be slightly less na\"ive and deal with the trend by fitting a quadratic term.

\end{frame}


\begin{frame}[fragile]
\frametitle{Multiplicative model with numerical explanatory variable}
\framesubtitle{Na\"ive price vs age models\ldots}
<<RC-H06-025, fig.show='hide'>>=
PriceAge.fit2=lm(price~age+I(age^2), data=Mazda.df)
plot(PriceAge.fit2,which=1)
@

<<RC-H06-026, echo=FALSE>>=
trimPlot(PriceAge.fit2,
         fileName = "figure/RC-H06-026.pdf",
         which = 1,
         cex = 0.7,
         cex.main = 0.7,
         fig.height = 2.5,
         fig.width = 4.5,
         x.lab = "Fitted values",
         y.lab = "Residuals",
         axis.size.cex = c(0.7, 0.525))
@

\begin{figure}
  \centering
  \includegraphics{figure/RC-H06-026}
\end{figure}

\end{frame}


\begin{frame}[fragile]
\frametitle{Multiplicative model with numerical explanatory variable}
\framesubtitle{Multiplicative price vs age model}
We have eliminated trend from these residuals but the \textbf{EOV} assumption is still violated.
Let us `tear up' this approach and take logs of price.

<<RC-H06-027, echo=FALSE>>=
trimPlot(log(price) ~ age,
         plotCommand = trendscatter,
         fileName = "figure/RC-H06-027.pdf",
         data = Mazda.df,
         main = "",
         x.lab = "Age",
         y.lab = "log (Price)",
         fig.height = 2.2,
         fig.width = 4.1,
         cex = 0.7)
@

\begin{figure}
  \centering
  \includegraphics{figure/RC-H06-027}
\end{figure}

This is something we have seen before, is it not?
\end{frame}

\begin{frame}[fragile, t]
\frametitle{Multiplicative model with numerical explanatory variable}
\framesubtitle{Multiplicative price vs age model\ldots}
Let us fit the more appropriate multiplicative model.
<<RC-H06-028, fig.show = 'hide'>>=
LogPriceAge.fit=lm(log(price)~age, data=Mazda.df)
plot(LogPriceAge.fit,which=1)
@

<<RC-H06-029, echo=FALSE>>=
trimPlot(LogPriceAge.fit,
         fileName = "figure/RC-H06-029.pdf",
         which = 1,
         cex = 0.7,
         cex.main = 0.7,
         fig.height = 2.1,
         fig.width = 4.1,
         x.lab = "Fitted values",
         y.lab = "Residuals")
@

\begin{figure}
  \centering
  \includegraphics{figure/RC-H06-029}
\end{figure}

\textbf{EOV} assumption is now satisfied. 

\end{frame}


\begin{frame}[fragile]
\frametitle{Multiplicative model with numerical explanatory variable}
\framesubtitle{Multiplicative price vsage model\ldots }
Check for normality of the residuals.
<<RC-H06-030, fig.show='hide'>>=
normcheck(LogPriceAge.fit)
@

<<RC-H06-031, echo=FALSE>>=
trimPlot(LogPriceAge.fit,
         fileName = "figure/RC-H06-031.pdf",
         plotCommand = normcheck,
         fig.height = 2.1,
         fig.width = 4.1
         )
@

\begin{figure}
  \centering
  \includegraphics[scale=0.5]{figure/RC-H06-031}
\end{figure}

Normality assumption seems justified.
\end{frame}


\begin{frame}[fragile]
\frametitle{Multiplicative model with numerical explanatory variable}
\framesubtitle{Multiplicative price vs age model\ldots}
Check for unduly influential data points.
  
<<RC-H06-032, fig.show='hide'>>=
cooks20x(LogPriceAge.fit)
@

<<RC-H06-033, echo=FALSE>>=
trimPlot(LogPriceAge.fit,
         fileName = "figure/RC-H06-033.pdf",
         plotCommand = cooks20x,
         fig.height = 2.25,
         fig.width = 4.25,
         mai = c(0.5, 0.6, 0.1, 0.1)
         )
@

\begin{figure}
  \centering
  \includegraphics[scale=0.5]{figure/RC-H06-033}
\end{figure}

It looks fine.
\end{frame}


\begin{frame}[fragile]
\frametitle{Multiplicative model with numerical explanatory variable}
\framesubtitle{Multiplicative price vs age model\ldots}
Our assumptions are sound, so we can trust the resulting output.

<<RC-H06-034, results='hide'>>=
summary(LogPriceAge.fit)
@

<<RC-H06-035, echo=FALSE>>=
slimSummary(LogPriceAge.fit)
@

<<RC-H06-036>>=
confint(LogPriceAge.fit)
@
There is a massively significant effect of age.
\bigskip

But how do we turn these estimated values into something meaningful?
\end{frame}


\begin{frame}[fragile]
\frametitle{Multiplicative model with numerical explanatory variable}
\framesubtitle{Inference from multiplicative price vs age model}
The above fitted model gives median log-price for a car of a given age.
So, exponentiating this model will give median price on the raw scale of \$A.

Using the equation for the fitted model, the estimated median prices is:
\begin{align*}
     \widehat{\rcode{price}}&=e^{\hat{\beta}_0+\hat{\beta}_1\times\rcode{age}} \\[-1mm]
                  &=e^{\hat{\beta}_0}e^{\hat{\beta}_1\times\rcode{age}}\\[-1mm]
                  &=e^{10.195210}\times e^{-0.163915 \times\rcode{age}} \\[-2mm]
                  &\approx \$26,775\times(0.85)^{\rcode{age}}
\end{align*}
This means that for every year the car ages it is worth .85 (85\% of) the previous year. That is, there is a 15\% decline in value. 
In accounting this is known as depreciation of an asset.

E.g., we estimate that the median price of a new Mazda is about \$26,775, 

a 1 year old about  $\$26,775\times 0.85^1\approx \$22,727$, and 

a 2 year old about  $\$26,775\times 0.85^2\approx \$19,291$\ldots
\end{frame}


\begin{frame}[fragile]
\frametitle{Multiplicative model with numerical explanatory variable}
\framesubtitle{Inference from multiplicative price vs age model}
\textbf{Note:} We exponentiate the model fitted to the logged Mazda prices
to obtain a model for Mazda price.\\
\bigskip\bigskip
Assumption checks, CIs, predictions (etc) use the model you fitted using the \rcode{lm} `machinery'. That is, on the log scale.\\
\bigskip\bigskip
The CIs and predictions are for use in the `real world', and
so must be back-transformed onto the scale of the raw data.
\end{frame}


\begin{frame}[fragile]
\frametitle{Multiplicative model with numerical explanatory variable}
\framesubtitle{CIs from multiplicative price vs age model}
We can obtain the confidence interval for median price of a new car 
by back-transforming the CI for the intercept value, just like we did with the null model discussed earlier.
\bigskip

<<RC-H06-037, echo=1>>=
exp(confint(LogPriceAge.fit))
ci = exp(confint(LogPriceAge.fit))
c1 = sprintf("%6.4e", ci[1,])
citmp = signif(ci[1,],5)
thou = floor(citmp / 1000)
hun = (signif(citmp, 3) - 1000 * thou) / 100
cidollar = sprintf("%2.0f,%-1.0f00", thou, hun)
@
\bigskip
The output:
\rcode{(Intercept)} \rcode{\Sexpr{c1[1]}} \rcode{\Sexpr{c1[2]}}~\footnote{\rcode{\Sexpr{c1[1]}}$=\Sexpr{signif(ci[1,1], 5)}=\$\Sexpr{format(citmp[1])}$} says that the we are confident that the median price of a new Mazda car 
in 1991 is somewhere between AUD\Sexpr{cidollar[1]} and AUD\Sexpr{cidollar[2]} (to the nearest \$100).
\end{frame}


\begin{frame}[fragile]
\frametitle{Multiplicative model with numerical explanatory variable}
\framesubtitle{CIs from multiplicative price vs age model\ldots}
<<RC-H06-038, echo=FALSE>>=
beta1 = coef(LogPriceAge.fit)[2]
ciBeta1 = exp(confint(LogPriceAge.fit))[2,]
@
Now let us look at the coefficient for \rcode{age} i.e. $\hat{\beta}_1$.\\
\medskip

If we exponentiate we get $e^{\hat{\beta}_1}=e^{\Sexpr{signif(beta1, 7)}} \approx \Sexpr{signif(exp(beta1), 2)}$ 
or using \rcode{R}:
<<RC-H06-039>>=
exp(coef(LogPriceAge.fit)[2])
@
\medskip

From the previous page we see that the 95\% CI for this coefficient is \Sexpr{sprintf("%7.5e",ciBeta1[1])}  to \Sexpr{sprintf("%7.5e",ciBeta1[2])}. That is, between \Sexpr{sprintf("%5.3f",ciBeta1[1])} and \Sexpr{sprintf("%5.3f",ciBeta1[2])}, say.
\bigskip

Alternatively, we can calculate the percentage rate of depreciation:
\medskip
<<RC-H06-039b>>=
100*(exp(confint(LogPriceAge.fit)[2,])-1)
@

This says that our 95\% CI for the annual depreciation in median price of Mazda cars is
between $100\% \times (1 - \Sexpr{sprintf("%5.3f",ciBeta1[2])}) =  \Sexpr{sprintf("%3.1f", 100 * (1 - ciBeta1[2]))}\%$ and $100\% \times (1 - \Sexpr{sprintf("%5.3f",ciBeta1[1])}) =  \Sexpr{sprintf("%3.1f", 100 * (1 - ciBeta1[1]))}\%$
\end{frame}


\begin{frame}[fragile]
\frametitle{Multiplicative model with numerical explanatory variable}
\framesubtitle{Predictions from multiplicative price vs age model}
Let us now construct prediction intervals for cars that are 0 -- 20 years old:
 
<<RC-H06-040, fig.show='hide'>>=
plot(price~age, col="light grey",data=Mazda.df)

## Create a data.frame which has car ages from 0 to 20
pred.df=data.frame(age=0:20) 

## Exponentiate the fitted values - in the same order as above
preds = exp(predict(LogPriceAge.fit,pred.df, interval="prediction"))

lines(0:20, preds[,"fit"],col="red") ## Predicted value
lines(0:20, preds[,"lwr"],col="blue", lty=2) ## Lower bound 
lines(0:20, preds[,"upr"],col="blue", lty=2) ## Upper bound 
@

\end{frame}


\begin{frame}[fragile]
\frametitle{Multiplicative model with numerical explanatory variable}
\framesubtitle{Predictions from multiplicative price vs age model\ldots}
<<RC-H06-041, echo=FALSE>>=
trimPlot(price ~ age,
         data = Mazda.df,
         fileName = "figure/RC-H06-041.pdf",
         fig.height = 2.5,
         fig.width = 4.5,
         x.lab = "Age",
         y.lab = "Price (AUD)",
         cex = 0.7,
         col = 'light grey',
         axis.size.cex = c(0.7, 0.525),
         addElements = list(
            lines(0:20, preds[,"fit"],col="red"),
            lines(0:20, preds[,"lwr"],col="blue", lty=2),
            lines(0:20, preds[,"upr"],col="blue", lty=2)
         ))
@

\begin{figure}
  \centering
  \includegraphics{figure/RC-H06-041}
\end{figure}

Notice how the fitted line is at the half way value (median) and is robust against extreme values.
\end{frame}


\begin{frame}[fragile]
\frametitle{Multiplicative model with numerical explanatory variable}
\framesubtitle{Executive summary: Mazda  price vs age}
<<RC-H06-042, echo=FALSE>>=
beta0 = coef(LogPriceAge.fit)[1]
beta1 = coef(LogPriceAge.fit)[2]
ciBeta1 = c(ciBeta1[1], exp(beta1), ciBeta1[2])
deprec = sprintf("%3.1f", 100 * (1 - ciBeta1))

ciBeta0 = exp(confint(LogPriceAge.fit)[1,])
ciBeta0 = c(ciBeta0[1], exp(beta0), ciBeta0[2])
citmp = signif(ciBeta0, 5)
thou = floor(citmp / 1000)
hun = (signif(citmp, 3) - 1000 * thou) / 100
cidollar = sprintf("%2.0f,%-1.0f00", thou, hun)
@
We were interested in how the price of Mazda cars changed as they aged.\\
\bigskip
We estimate that the median value of a new car is about \$\Sexpr{cidollar[2]} (we are confident the median is somewhere between \$\Sexpr{cidollar[1]} and \$\Sexpr{cidollar[3]}) and that for every year the car ages the median price depreciates at about \Sexpr{deprec[2]}\% per year, and we are confident it is somewhere between \Sexpr{deprec[1]} and \Sexpr{deprec[3]}\% 
\bigskip
\begin{columns}
\begin{column}{0.7\textwidth}
Take home message: 

\textbf{Do not buy new cars!!!}...unless you can truly afford the depreciation hit.
\end{column}
\begin{column}{0.3\textwidth}
\includegraphics[width=1.25in]{LosingMoney.jpg}
\end{column}
\end{columns}

\end{frame}


\begin{frame}[fragile]
\frametitle{Mazda price vs.\ age}
\framesubtitle{Multi-year depreciation}
Most of us keep our cars for more than a year, so we might be interested to know how much Mazdas depreciate over a 5 year period, say.\\
\bigskip
The CI for 5-year depreciation is obtained by raising the CI (for 1-year depreciation)
to the power of 5. This is an intuitive thing to do, as depreciation acts multiplicatively.
<<RC-H06-043>>=
exp(confint(LogPriceAge.fit)[2,])^5
@
\medskip
As a percentage change this is
<<RC-H06-044, echo=1>>=
100*(exp(confint(LogPriceAge.fit)[2,])^5-1)
ci = sprintf("%4.1f", abs(100*(exp(confint(LogPriceAge.fit)[2,])^5-1)))
@
\medskip
Ouch, the median price of Mazdas drops between \Sexpr{ci[2]}\% and \Sexpr{ci[1]}\% over 5 years.
\end{frame}


\begin{frame}[fragile]
\frametitle{Mazda  price vs.\ age\ldots}
\framesubtitle{Multi-year depreciation\ldots}
Equivalently, the above CI can be obtained by back-transforming using five times the estimated effect.
\bigskip
<<RC-H06-045>>=
exp(5*confint(LogPriceAge.fit)[2,])
@
\bigskip
As a percentage change this is
\medskip
<<RC-H06-046>>=
100*(exp(5*confint(LogPriceAge.fit)[2,])-1)
@
\end{frame}
%\end{comment}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\BeginSection{Example 2: Multiplicative model with categorical explanatory variable}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{frame}[fragile]
\frametitle{Multiplicative model with categorical explanatory variable}
\framesubtitle{Trawl bycatch}

It was of interest to compare the amount of bycatch caught by two types of fishing trawl.

\begin{figure}
  \centering
  \includegraphics[width=0.65\textwidth]{DemersalTrawling.jpg} 
\end{figure}

\textbf{Intuition:} Would you expect the effect of trawl type to be additive or multiplicative?

\end{frame}


\begin{frame}[fragile]
\frametitle{Multiplicative model with categorical explanatory variable}
\framesubtitle{Trawl by-catch\ldots}
<<RC-H06-047, fig.show='hide'>>=
Bycatch.df=read.table("Data/Bycatch.txt",header=T)
boxplot(Bycatch~Trawl,data=Bycatch.df, horizontal=T,xlab="Bycatch (kg)")
@

<<RC-H06-048, echo=FALSE>>=
trimPlot(Bycatch ~ Trawl,
         data = Bycatch.df,
         fileName = "figure/RC-H06-048.pdf",
         plotCommand = boxplot,
         horizontal = TRUE,
         fig.height = 1.8,
         fig.width = 4.5,
         cex = 0.7,
         x.lab = "Bycatch (kg)",
         y.lab = "Trawl",
         lasy = 0,
         mai = c(0.45, 0.5, 0.1, 0.1),
         .at = c(1, 2),
         .labels = c("Standard", "Tapered"))
@

\begin{figure}
  \centering
  \includegraphics{figure/RC-H06-048}
\end{figure}

<<RC-H06-049>>=
summaryStats(Bycatch~Trawl,data=Bycatch.df)
@
\end{frame}


\begin{frame}[fragile]
\frametitle{Multiplicative model with categorical explanatory variable}
\framesubtitle{Trawl by-catch\ldots}

This seems to confirm our intuition that these data should be modelled on the log scale.
We will play stupid for now, and fit a linear model to the raw data....

<<RC-H06-050, fig.show='hide'>>=
Trawl.lm=lm(Bycatch~Trawl,data=Bycatch.df)
plot(Trawl.lm,which=1)
@

% \end{frame}


% \begin{frame}[fragile]
% \frametitle{Multiplicative model with categorical explanatory variable}
% \framesubtitle{Trawl bycatch}

<<RC-H06-051, echo=FALSE>>=
trimPlot(Trawl.lm,
         fileName = "figure/RC-H06-051.pdf",
         which = 1,
         cex = 0.7,
         cex.main = 0.7,
         fig.height = 1.8,
         fig.width = 3.6,
         x.lab = "Fitted values",
         y.lab = "Residuals")
@

\begin{figure}
  \centering
  \includegraphics{figure/RC-H06-051}
\end{figure}

The \textbf{EOV} assumption is very doubtful. Normality?
\end{frame}


\begin{frame}[fragile]
\frametitle{Multiplicative model with categorical explanatory variable}
\framesubtitle{Trawl by-catch\ldots}
<<RC-H06-052, fig.show='hide'>>=
normcheck(Trawl.lm)
@

<<RC-H06-053, echo=FALSE>>=
trimPlot(Trawl.lm,
         plotCommand = normcheck,
         fileName = "figure/RC-H06-053.pdf",
         fig.height = 2.1,
         fig.width = 4.1)
@

\begin{figure}
  \centering
  \includegraphics[scale=0.5]{figure/RC-H06-053}
\end{figure}

These plots of the residuals confirm that \textbf{EOV} and normality do not hold.
\end{frame}


\begin{frame}[fragile]
\frametitle{Multiplicative model with categorical explanatory variable}
\framesubtitle{Using logged trawl by-catch}
<<RC-H06-054, fig.show='hide'>>=
boxplot(log(Bycatch)~Trawl,data=Bycatch.df,horizontal=T,xlab="log(Bycatch)")
@

<<RC-H06-055, echo=FALSE>>=
trimPlot(log(Bycatch) ~ Trawl,
         data = Bycatch.df,
         fileName = "figure/RC-H06-055.pdf",
         plotCommand = boxplot,
         horizontal = TRUE,
         fig.height = 1.8,
         fig.width = 4.5,
         cex = 0.7,
         x.lab = "log(Bycatch)",
         y.lab = "Trawl",
         lasy = 0,
         mai = c(0.45, 0.5, 0.1, 0.1),
         .at = c(1, 2),
         .labels = c("Standard", "Tapered"))
@

\begin{figure}
  \centering
  \includegraphics{figure/RC-H06-055}
\end{figure}

Looking much better. 
\end{frame}


\begin{frame}[fragile]
\frametitle{Multiplicative model with categorical explanatory variable}
\framesubtitle{Using logged trawl by-catch\ldots}

<<RC-H06-056, fig.show='hide'>>=
Trawl.lmlog=lm(log(Bycatch)~Trawl,data=Bycatch.df)
plot(Trawl.lmlog,which=1)
@

<<RC-H06-057, echo=FALSE>>=
trimPlot(Trawl.lmlog,
         fileName = "figure/RC-H06-057.pdf",
         which = 1,
         cex = 0.7,
         cex.main = 0.7,
         fig.height = 2.1,
         fig.width = 4.1,
         x.lab = "Fitted values",
         y.lab = "Residuals")
@

\begin{figure}
  \centering
  \includegraphics{figure/RC-H06-057}
\end{figure}

Sweet as.
\end{frame}


\begin{frame}[fragile]
\frametitle{Multiplicative model with categorical explanatory variable}
\framesubtitle{Using logged trawl by-catch\ldots}
<<RC-H06-058, fig.show='hide'>>=
normcheck(Trawl.lmlog)
@

<<RC-H06-059, echo=FALSE>>=
trimPlot(Trawl.lmlog,
         plotCommand = normcheck,
         fileName = "figure/RC-H06-059.pdf",
         fig.height = 2.1,
         fig.width = 4.1)
@

\begin{figure}
  \centering
  \includegraphics[scale=0.5]{figure/RC-H06-059}
\end{figure}

Looking good. 
\end{frame}


\begin{frame}[fragile]
\frametitle{Multiplicative model with categorical explanatory variable}
\framesubtitle{Using logged trawl by-catch\ldots}
<<RC-H06-060, fig.show = 'hide'>>=
cooks20x(Trawl.lmlog)
@

<<RC-H06-061, echo=FALSE>>=
trimPlot(Trawl.lmlog,
         plotCommand = cooks20x,
         fileName = "figure/RC-H06-061.pdf",
         fig.height = 2.1,
         fig.width = 4.1)
@

\begin{figure}
  \centering
  \includegraphics[scale=0.5]{figure/RC-H06-061}
\end{figure}

No problems. 
\end{frame}



\begin{frame}[fragile]
\frametitle{Multiplicative model with categorical explanatory variable}
\framesubtitle{Results: Using logged trawl by-catch}
Assumptions are satisfied. We can trust the fitted model.

<<RC-H06-062, results='hide'>>=
summary(Trawl.lmlog)
@

<<RC-H06-063, echo=F>>=
slimSummary(Trawl.lmlog)
p = summary(Trawl.lmlog)$coefficients[2,4]
r2 = summary(Trawl.lmlog)$r.squared
@
There is a statistically significance effect of trawl type (\pval{} $\approx \Sexpr{round(p, 3)}$).

However, our model only explained \Sexpr{round(100 * r2, 0)}\% of the variability in the logged data and 
will not be very good for prediction.
\end{frame}


\begin{frame}[fragile]
\frametitle{Multiplicative model with categorical explanatory variable}
\framesubtitle{Interpretation: Using logged trawl by-catch}
We need to back-transform to the raw data scale to make sense of the above results:
\medskip

<<RC-H06-064, echo = 1>>=
exp(confint(Trawl.lmlog))
ci = round(100 * exp(confint(Trawl.lmlog)[2,]))
@
\medskip
The median by-catch using the tapered trawl was estimated to be between \Sexpr{ci[1]}\% and \Sexpr{ci[2]}\% that of the standard trawl.
\bigskip

Alternatively, 
<<>>=
100*(exp(confint(Trawl.lmlog)[2,])-1)
@
the median by-catch using the tapered trawl was estimated to be between \Sexpr{100-ci[2]}\% and \Sexpr{100-ci[1]}\% smaller than when using the standard trawl.
\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\BeginSection{Closing remarks and relevant \rcode{R}-code}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{frame}[fragile]
\frametitle{Closing remarks}
\textbf{NOTE:} Any of the types of linear model that you encounter {\em could} be
applied to logged data if that is more appropriate, either due to rationale and/or 
due to right-skewness in the data.\\
\bigskip
In all cases you will follow the same procedure:\\
\bigskip
\begin{enumerate}[1.]\setlength{\itemsep}{5mm}
\item Fit the linear model to the logged data.
\item Back-transform (i.e., exponentiate) the relevant estimated coefficients, and confidence or 
prediction intervals.
\item \textbf{Remember} that the effect is multiplicative, 
and that back-transformed coefficients and CIs are for the median.
\end{enumerate}
\end{frame}



\begin{frame}[fragile]

\frametitle{Most of the new \rcode{R}-code you need for this chapter}
If any of the following hold:
\begin{itemize}
\item A multiplicative effect of the explanatory variables seems appropriate
\item Right skewness of the variability (inspect the residuals to check)
\item A funnel effect in the plot of residuals vs fitted values
\end{itemize}
then a log transformation of the response variable $y$ may be appropriate. The usual steps (fitting a linear model and assumption checking, CIs, etc) are then applied to the logged response.


<<RC-H06-065, eval=F, comment=NA>>= 
Trawl.lmlog=lm(log(Bycatch)~Trawl,data=Bycatch.df)
# then check if it's okay
plot(Trawl.lmlog,which=1)
@


Confidence intervals can be back-transformed and interpreted as a multiplicative increases/decreases - in this case relative to baseline:


<<RC-H06-066, echo = 1>>=
exp(confint(Trawl.lmlog))
ci = round(100 * exp(confint(Trawl.lmlog)[2,]))
@

and we interpret this with respect to change in the median value of $y$.

\end{frame}

\end{document}

